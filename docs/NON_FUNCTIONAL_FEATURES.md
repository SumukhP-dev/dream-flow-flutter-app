# Non-Functional Features (Excluding Stripe Payments)

This document lists all non-functional or partially implemented features that are NOT related to Stripe payments.

## üî¥ High Priority: Broken/Incomplete Features

### 1. On-Device Image Generation (Android & iOS)

**Status**: ‚úÖ FULLY IMPLEMENTED - Stable Diffusion pipeline complete

**Location**:

- `dream-flow-app/app/lib/core/models/image_model.dart` (lines 145-450) ‚úÖ IMPLEMENTED
- `dream-flow-app/app/android/app/src/main/kotlin/com/example/dream_flow/MainActivity.kt` (lines 161-202, 250-293) ‚úÖ UPDATED
- `dream-flow-app/app/ios/Runner/AppDelegate.swift` (lines 345-399, 401-437) ‚úÖ UPDATED

**Implementation**:

‚úÖ **Complete Stable Diffusion Pipeline**:
1. ‚úÖ **Text Encoder**: Converts prompt to embeddings using GPT-2 tokenizer
   - Tokenizes prompt (max 77 tokens)
   - Runs text encoder model to get embeddings [1, 77, 768]
   
2. ‚úÖ **UNet Denoising Loop**: Iterative denoising with DDIM scheduler
   - Initializes random noise latents [1, 4, 64, 64]
   - Runs UNet for each timestep (numInferenceSteps)
   - Applies DDIM scheduler to denoise latents
   
3. ‚úÖ **VAE Decoder**: Converts latents to image pixels
   - Takes final denoised latents [1, 4, 64, 64]
   - Decodes to image [1, 3, 512, 512] (RGB)
   
4. ‚úÖ **Post-processing**: Normalizes and converts to uint8 image bytes
   - Converts from [-1, 1] to [0, 255]
   - Reshapes to [H, W, 3] format
   - Returns as PNG/JPEG bytes

5. ‚úÖ **Multiple Images**: Generates multiple images with different seeds
   - Each image uses unique seed (timestamp + index)
   - Processes sequentially (can be parallelized later)

**Current Status**:

- ‚úÖ Full pipeline implemented in Dart (`image_model.dart`)
- ‚úÖ Android native code updated to load 3 separate models (text encoder, UNet, VAE)
- ‚úÖ iOS native code updated (ready for Core ML implementation)
- ‚ö†Ô∏è **Note**: Actual model files must be provided (text_encoder.tflite, unet.tflite, vae_decoder.tflite)
- ‚ö†Ô∏è **Note**: Model input/output shapes may need adjustment based on actual model format
- ‚ö†Ô∏è **Testing Required**: Needs testing with actual Stable Diffusion TFLite/Core ML models

**Files Updated**:

- ‚úÖ `dream-flow-app/app/lib/core/models/image_model.dart` - Full pipeline implementation
- ‚úÖ `dream-flow-app/app/android/app/src/main/kotlin/com/example/dream_flow/MainActivity.kt` - Model loading updated
- ‚úÖ `dream-flow-app/app/ios/Runner/AppDelegate.swift` - Ready for Core ML models

---

### 2. Android TFLite Story Generation

**Status**: ‚úÖ Implementation improved - better error handling added, needs testing

**Location**: `dream-flow-app/app/lib/core/models/story_model.dart` (lines 207-316)

**Issue**:

- ‚úÖ Model loading is enabled with improved error handling
- ‚úÖ Added detailed logging for debugging failures
- ‚úÖ Improved error messages indicating what went wrong
- ‚ö†Ô∏è Falls back to `_generatePlaceholderStory()` on errors (by design for graceful degradation)
- ‚ö†Ô∏è TFLite interpreter inference loop implemented but needs testing on physical device

**Current Behavior**:

- ‚úÖ Attempts to load TFLite model with clear error messages if missing
- ‚úÖ Runs inference if model loads successfully
- ‚úÖ Falls back to placeholder text on any error with detailed logging
- ‚úÖ Better error messages help diagnose issues (missing models, tokenizer problems, inference errors)

**Required Testing**:

1. ‚ö†Ô∏è Test actual TFLite model inference on physical Android device
2. ‚ö†Ô∏è Verify tokenizer integration works correctly
3. ‚ö†Ô∏è Ensure model output format matches expected shape
4. ‚úÖ Better error handling and logging added

---

### 3. iOS Core ML Story Generation

**Status**: ‚úÖ Platform channel and GPT2Tokenizer implemented - needs testing

**Location**:

- `dream-flow-app/app/lib/core/models/story_model.dart` (lines 325-353)
- `dream-flow-app/app/ios/Runner/AppDelegate.swift` (lines 106-308)
- `dream-flow-app/app/ios/Runner/GPT2Tokenizer.swift` ‚úÖ EXISTS

**Issue**:

- ‚úÖ Platform channel implementation exists and is complete
- ‚úÖ `GPT2Tokenizer` Swift class exists and is implemented
- ‚úÖ Fixed bug in GPT2Tokenizer.fromJSON() (line 55 was assigning wrong value)
- ‚ö†Ô∏è Needs testing on physical iOS device with Neural Engine
- ‚ö†Ô∏è Requires Core ML model file (.mlmodel or .mlpackage) in bundle

**Required Testing**:

1. ‚úÖ `GPT2Tokenizer.swift` exists and is properly implemented (fixed)
2. Ensure Core ML model format is correct (.mlmodel or .mlpackage)
3. Test on physical iOS device with Neural Engine (A12 Bionic or later)
4. Verify tokenizer files are included in app bundle

---

### 4. Klaviyo Flow Creation

**Status**: ‚úÖ Documented limitation - Klaviyo API doesn't support programmatic flow creation

**Location**: `backend_fastapi/app/dreamflow/klaviyo_service.py` (lines 882-913)

**Issue**:

- ‚úÖ `create_flow()` method documented - Klaviyo REST API does not support programmatic flow creation
- ‚úÖ Added comprehensive documentation explaining the limitation
- ‚úÖ `trigger_flow()` works with manually created flows (already implemented)

**Current Behavior**:

- `create_flow()` returns `None` by design - Klaviyo REST API limitation
- Flows must be created manually in Klaviyo dashboard
- `trigger_flow()` can trigger flows that exist in dashboard
- Event-based triggers (recommended) work automatically when events are tracked

**Required Action**:

1. ‚úÖ Documented that flows must be created via Klaviyo dashboard (done)
2. ‚úÖ Added clear documentation in code comments (done)
3. ‚úÖ `trigger_flow()` already works with manually created flows (verified)
4. ‚ö†Ô∏è Users must create flows in Klaviyo dashboard before using `trigger_flow()`

---

### 6. On-Device ML Service Fallbacks

**Status**: Uses placeholder implementations when models fail

**Location**: `dream-flow-app/app/lib/core/on_device_ml_service.dart`

**Issue**:

- `generateStory()` falls back to `_generatePlaceholderStory()` if model loader fails
- `generateImages()` returns empty list `[]` if image model loader fails
- `generateAudio()` returns empty `Uint8List(0)` if TTS service fails

**Current Behavior**:

- All methods have fallback placeholders
- Service doesn't crash but returns non-functional results

**Impact**:

- App continues to work but with degraded functionality
- Users get placeholder content instead of AI-generated content

---

## üü° Medium Priority: Partially Implemented Features

### 7. GPT2Tokenizer Swift Implementation

**Status**: ‚úÖ EXISTS and IMPLEMENTED - Fixed bug in fromJSON method

**Location**: `dream-flow-app/app/ios/Runner/GPT2Tokenizer.swift` ‚úÖ EXISTS

**Issue**:

- ‚úÖ `GPT2Tokenizer.swift` file exists and is fully implemented
- ‚úÖ Fixed bug in `fromJSON()` method (line 55 was assigning `id` instead of `key` to `idToToken`)
- ‚úÖ Supports loading from bundle or documents directory
- ‚úÖ Has fallback simple tokenizer for when files are missing
- ‚ö†Ô∏è Needs testing with actual tokenizer.json files

**Required Testing**:

1. ‚úÖ File exists and is properly implemented (verified)
2. ‚úÖ Properly loads Hugging Face tokenizer format (JSON structure correct)
3. ‚ö†Ô∏è Test tokenization and detokenization functions with real data
4. ‚ö†Ô∏è Verify tokenizer.json files are included in app bundle

---

### 8. Native Mobile Visual Generator

**Status**: Placeholder implementation

**Location**: `backend_fastapi/app/core/native_mobile_services.py` (lines 80-236)

**Issue**:

- `NativeMobileVisualGenerator` exists but relies on fallback implementations
- May not actually use native mobile models for image generation

**Current Behavior**:

- Falls back to local services when native ML server is unavailable
- Not clear if native mobile visual generation is actually used

---

### 9. Native Mobile Narration Generator

**Status**: Placeholder that falls back to local TTS

**Location**: `backend_fastapi/app/core/native_mobile_services.py` (lines 239-272)

**Issue**:

- `NativeMobileNarrationGenerator` exists but always falls back to `LocalNarrationGenerator`
- Comment says "TTS is typically handled by native APIs" but code uses local TTS
- Not actually using native mobile TTS

**Required Implementation**:

- Should call mobile ML server or use platform channels for native TTS
- Currently just a wrapper around local TTS

## Summary: What Needs Code Fixes

### Critical (Returns Empty/Placeholder Results):

1. ‚úÖ **TTS Service** - FIXED (was placeholder, now implemented)
2. ‚úÖ **On-Device Image Generation** - IMPLEMENTED (full Stable Diffusion pipeline, needs model files)
3. ‚úÖ **Android TFLite Story Generation** - IMPROVED (better error handling, needs testing)
4. ‚úÖ **iOS Core ML Story Generation** - VERIFIED (GPT2Tokenizer exists and fixed, needs testing)
5. ‚úÖ **Klaviyo Flow Creation** - DOCUMENTED (API limitation, not a bug)

### Medium (Partial Implementation):

6. ‚ö†Ô∏è **Native Mobile Visual Generator** - Falls back to local services
7. ‚ö†Ô∏è **Native Mobile Narration Generator** - Always uses local TTS instead of native

### Working but Disabled (Not Bugs):

8. ‚úÖ **Video Generation** - Working code, intentionally disabled
9. ‚úÖ **YouTube Automation** - Working code, needs credentials
10. ‚úÖ **Google Cloud Vertex AI** - Working code, needs credentials
11. ‚úÖ **Apple Intelligence API** - Future feature, API not available yet

---

## Quick Reference

**Immediate Action Required**:

1. ‚úÖ **Stable Diffusion pipeline** - IMPLEMENTED (needs model files: text_encoder.tflite, unet.tflite, vae_decoder.tflite)
2. ‚úÖ **GPT2Tokenizer Swift implementation** - Verified and fixed (needs testing)
3. ‚úÖ **Klaviyo Flow creation** - Documented limitation (not a bug, API doesn't support it)
4. ‚ö†Ô∏è **Test Android TFLite story generation** on physical devices to verify it works
5. ‚ö†Ô∏è **Test iOS Core ML story generation** on physical devices to verify it works
6. ‚ö†Ô∏è **Test Stable Diffusion pipeline** with actual model files on physical devices

**Can Wait**:

- Native mobile services (fallback works)
- Video generation (disabled intentionally)
- Optional services (need credentials only)
