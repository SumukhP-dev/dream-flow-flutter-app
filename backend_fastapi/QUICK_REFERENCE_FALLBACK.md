# AI Inference Fallback - Quick Reference

## TL;DR

The backend now has **graceful fallback** from Cloud HuggingFace ‚Üí Server Local Models ‚Üí Native Mobile AI chips.

## What Was Fixed

‚úÖ Missing configuration properties added (`use_local_models`, `ollama_*`)  
‚úÖ Fallback order now matches documentation  
‚úÖ Runtime fallback respects `AI_INFERENCE_MODE` configuration  
‚úÖ Visual generation has proper fallback chain  
‚úÖ Comprehensive logging for debugging  

## Quick Start

### Recommended Production Setup
```env
AI_INFERENCE_MODE=cloud_first
HUGGINGFACE_API_TOKEN=hf_xxxxx
LOCAL_INFERENCE=true  # Fallback
```

### Development Setup
```env
AI_INFERENCE_MODE=server_first
LOCAL_INFERENCE=true
HUGGINGFACE_API_TOKEN=hf_xxxxx  # Optional fallback
```

## Available Modes

| Mode | Fallback? | Order |
|------|-----------|-------|
| `cloud_first` | ‚úÖ | Cloud ‚Üí Local ‚Üí Mobile |
| `server_first` | ‚úÖ | Local ‚Üí Cloud ‚Üí Mobile |
| `phone_first` | ‚úÖ | Mobile ‚Üí Local ‚Üí Cloud |
| `cloud_only` | ‚ùå | Cloud (no fallback) |
| `server_only` | ‚ùå | Local (no fallback) |
| `phone_only` | ‚ùå | Mobile (no fallback) |

## Testing Fallback

### Test 1: Cloud to Local
```bash
# Set invalid token to force fallback
AI_INFERENCE_MODE=cloud_first
HUGGINGFACE_API_TOKEN=invalid
LOCAL_INFERENCE=true

# Check logs for:
# "‚ùå Failed to initialize cloud generators"
# "üîÑ Falling back to next option in chain"
# "üíª Using LOCAL inference mode"
```

### Test 2: Local to Cloud
```bash
# Don't install llama-cpp-python
AI_INFERENCE_MODE=server_first
HUGGINGFACE_API_TOKEN=hf_xxxxx

# Check logs for:
# "‚ùå Failed to initialize local generators"
# "üîÑ Falling back to next option in chain"
# "üåê Using CLOUD inference mode"
```

## Common Issues

### "AttributeError: 'Settings' object has no attribute 'use_local_models'"
**Fixed!** This was a bug in the old code. Update to latest version.

### "All inference options failed"
**Solution**: Ensure at least one mode is properly configured:
- Cloud: Valid `HUGGINGFACE_API_TOKEN`
- Local: `LOCAL_INFERENCE=true` + model files exist
- Mobile: Flutter ML server running on port 8081

### Fallback not working
**Check**:
1. Using fallback mode? (`*_first`, not `*_only`)
2. Fallback option is configured?
3. Check logs for "Fallback disabled" messages

## Logs to Watch

```
# Initialization
üåê Using CLOUD inference mode (HuggingFace APIs)
üíª Using LOCAL inference mode (CPU-optimized)
üì± Using native mobile inference mode (TFLite/Core ML)

# Fallback
‚ùå Failed to initialize cloud generators: [error]
üîÑ Falling back to next option in chain...
‚úÖ Generators initialized successfully

# Runtime fallback
Primary story generator failed: TimeoutError
Attempting fallback to local story generator
‚úÖ Using fallback story generator
```

## Files Modified

- `app/shared/config.py` - Added Ollama config
- `app/dreamflow/main.py` - Refactored fallback logic
- `AI_INFERENCE_MODES.md` - Updated documentation
- `FALLBACK_SYSTEM_AUDIT_AND_FIXES.md` - Detailed audit report
- `docs/FALLBACK_FLOW_DIAGRAM.md` - Visual flow diagrams

## Next Steps

1. Test in staging with real workloads
2. Monitor fallback frequency (high rate = config issue)
3. Adjust fallback chains based on performance data
4. Consider circuit breaker for repeated failures

## Questions?

See full documentation:
- `FALLBACK_SYSTEM_AUDIT_AND_FIXES.md` - Complete audit report
- `docs/FALLBACK_FLOW_DIAGRAM.md` - Visual flow diagrams
- `AI_INFERENCE_MODES.md` - Configuration guide

---

**Last Updated**: January 11, 2026  
**Status**: ‚úÖ Ready for testing
